---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a third-year Master student at the Large Model Research Center, Institute of Automation, Chinese Academy of Sciences, under the guidance of Associate Professors  [Dr. Zhu](https://scholar.google.com/citations?user=tVeCxZcAAAAJ&hl=en&oi=ao) and [Dr. Yi](https://scholar.google.com/citations?user=iga8Z4AAAAAJ&hl=en&oi=ao). I am also supervised by [Prof. Wang](https://scholar.google.com/citations?user=7_BkyxEAAAAJ&hl=en). I got my Bachelor's degree from Department of Automation, Tsinghua University. My undergraduate thesis about Operations Optimization was completed under the supervision of [Prof. Song](https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=en).

My research interests include multimodal deep learning, with a particular focus on vision-language alignment, large language models, benchmarks, and dataset development. I have completed three comprehensive research projects, each culminating in a paper<img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations">. I also have sufficient experence dealing with MLLM's coding environment and finetuning.  

(2025.01) NOW I am seeking a PhD position in Fall, 2025. I am also looking for a Research Assistant position in this year.


# üî• News
- *2024.12*: &nbsp; Review   üéâüéâ A paper about *Video Large Models* is Submitted to Anonymous Review.
- *2024.06*: &nbsp; ArXiv&nbsp;    üéâüéâ *Recurrent Context Compression: Efficiently Expanding the Context Window of LLM* is now on ArXiv.
- *2022.05*: &nbsp; NIPS2022 üéâüéâ *Taisu: A 166m large-scale high-quality dataset for chinese vision-language pre-training* is Published.


# üìù Preprints

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">Reviewing</div>
      <img src='images/anonymous_resized.png' alt="sym" width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
   <strong>A Video Large Model Benchmark on Anomaly Analysis</strong></a><br>
    <strong>Haoran Chen</strong>, Dong Yi, Moyan Cao, Chensen Huang, Guibo Zhu, Jinqiao Wang<br><br>
    This benchmark introduces a large-scale dataset for evaluating multimodal large language models (MLLMs) in crime surveillance video analysis. It introduces various question-answering tasks, employs GPT-4o for open-ended evaluation, and highlights MLLM improvements after fine-tuning, advancing anomaly analysis research.
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">ArXiv</div>
      <img src='images/rcc_resized.png' alt="sym" width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
    <a href="https://arxiv.org/pdf/2406.06110"><strong>Recurrent Context Compression: Efficiently Expanding the Context Window of LLM</strong></a><br>
    Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, <strong>Haoran Chen</strong>, Dong Yi, Jinqiao Wang<br><br>
    The paper introduces Recurrent Context Compression (RCC), a novel method to extend context windows in Transformer-based large language models efficiently. RCC uses an autoencoder structure and a two-stage training process to compress long texts while retaining high accuracy. The approach achieves competitive performance in long-text tasks, improves memory efficiency, and addresses challenges with instruction reconstruction.
  </div>
</div>

# üìù Publications

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">NIPS2022</div>
      <img src='images/taisu500x300.png' alt="sym" width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
    <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf"><strong>TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training</strong></a><br>
    Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, <strong>Haoran Chen</strong>, Guanhui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang<br><br>
    The TaiSu dataset introduces a large-scale, high-quality Chinese vision-language pre-training resource with 166 million images and 219 million captions. It leverages an automatic data acquisition and filtering framework. TaiSu outperforms existing benchmarks in zero-shot image-text retrieval, classification, and generative tasks, addressing limitations in prior Chinese cross-modal datasets.
  </div>
</div>


# üìñ Educations
- *2022.09 - 2025.06 (now)*, Master, Institute of Automation, Chinese Academy of Sciences
- *2018.09 - 2022.06*, Bachelor, Department of Automation, School of Information Science and Techonology, Tsinghua University


# üíª Internships
-  *2023.09 - 2024,12*, Student Researcher in Wuhan Artificial Intelligence Research, Wuhan.
-  *2021.11 - 2022.01*, Researcher on Fire Recoginition in XinAo Group, Beijing.
-  *2021.06 - 2021.10*, Game Develop in Bytedance, Beijing.


# üèÄ Interest
- I found not only my passion in scientific research, but also in a colorful lifestyle.
- I love playing basketball and tennis. I routinely work out in gym. 
- I love traveling and citywalk. I am experienced at Landscape Photography.
- I love Chinese traditional culture and read ancient Chinese language a lot.
- I love music, especially pop and piano.
